= Structured logging with SLF4J and Logback
:published_at: 2017-12-01
:hp-tags: java
:hp-image: /images/logos/slf4j.png

I stumbled upon the term "structured logging" reading https://kartar.net/2015/12/structured-logging/[a 2015 blog post by James Turnbull].
It's not a new concept. 
I discovered Python developpers had a library dedicated to *structured logging*: http://www.structlog.org[structlog].

In this article I will describe what can be done in Java with usual logging libraries like SLF4J et Logback.

== Structured logging with SLF4J

All Java developpers know how to log a message:
[source,java]
----
Logger demoLogger = LoggerFactory.getLogger("logodissey.DemoLogger");
demoLogger.info("Hello world!");
----

Properly configured, it produces a log like
[source]
----
21:10:29.178 [Thread-1] INFO  logodissey.DemoLogger - Hello world!
----
Notice, this "Hello world!" message is qualified with several fields:

* Timestamp
* Thread Id
* Level
* Logger/Category

We can add more contextual information to this list, and give more detail about what was going on when this log was printed. For instance, to log the user Id, let's use the MDC (Mapped Diagnostic Context) to set more data in the log:
[source,java]
----
MDC.put("userId", "gquintana");
demoLogger.info("Hello world!");
MDC.remove("userId");
----
With the suitable configuration, we can get the user Id the log:
[source]
----
21:10:29.178 [Thread-1] gquintana INFO  logodissey.DemoLogger - Hello world!
----
The MDC can store information about the user (user Id, session Id, token Id), about the current request (request Id, transaction Id), about long running threads (batch instance Id, broker client Id). 
Later on, this information will be part of the log.

Having this kind of information allows to group logs by user, by request, by processing.
Remember that logs may be scattered across differents servers, on different time periods. 
In short having something to correlate logs is important.

Let's get back to the example.
The MDC is stores extra information about logs.
It is usually based on a thread local variable, this  means two things:

1. It must be properly cleaned after being used, or you may experience information leaks if the thread is reused (thread pools).
2. The information may no be properly transfered from thread to another. Think about asynchronous calls.

As a result calling `MDC.remove`, like the above example, (or `MDC.clear`) is required to clean the MDC after usage.
To avoid forgetting to do housekeeping afterwards, we can use a try-with-resource:
[source,java]
----
try(MDC.MDCCloseable mdc = MDC.putCloseable("user", "gquintana")) {
	demoLogger.info("Hello world!");
}
----
Hopefully, this kind of code won't make its way in your business code because, it is usually hidden in an interceptor like a Servlet filter, a Spring aspect. In Logback, there is a `MDCInsertingServletFilter` class which can be used as an example.


== JSON logging with Logback

At this point, a log is more than simple string, 
it's qualified with useful information: timestamp, level, thread, user, message.
In order to send it to a log collection tool like Elasticsearch, 
you should keep the in a structured format like JSON.
You should not flatten it in you own specific text format.

A structured format also has the virtue of handling properly multiline logs like stack traces, call traces and messages containing line separators (wanted or not).

Most popular log collection tools likes Filebeat, Graylog, Fluent already use some kind of compressed JSON format under the hood.
You should too.

Generating JSON logs with Logback is very easy. 
I'll show how to use two Logback extensions, 
the https://github.com/logstash/logstash-logback-encoder[Logstash Logback encoder] 
and the https://github.com/qos-ch/logback-contrib/wiki[Logback Contrib] library.

The first one uses a Logback extension point known as *encoder* that you can plug into any appender:
[source,xml]
----
    <appender name="FILE" class="ch.qos.logback.core.FileAppender">
        <file>log/log-odissey.log</file>
        <encoder class="net.logstash.logback.encoder.LogstashEncoder">
            <customFields>{"application":"log-odissey"}</customFields>
        </encoder>
    </appender>
----
It will produce the expected result:
[source,json]
----
{
  "@timestamp": "2017-11-25T21:10:29.178+01:00",
  "@version": 1,
  "message": "Hello world!",
  "logger_name": "logodissey.DemoLogger",
  "thread_name": "Thread-1",
  "level": "INFO",
  "level_value": 20000,
  "HOSTNAME": "my-laptop",
  "userId": "gquintana",
  "application": "log-odyssey"
}
----
The Maven coordinates for this library are `net.logstash.logback:logstash-logback-encoder:4.11`.

The second one uses a different extention point called *layout*.
In the end, it looks very similar to the first one, a bit more verbose though:
[source,xml]
----
    <appender name="FILE
    "class="ch.qos.logback.core.FileAppender">
        <file>target/log/log-odissey.log</file>
        <encoder class="ch.qos.logback.core.encoder.LayoutWrappingEncoder">
            <layout class="ch.qos.logback.contrib.json.classic.JsonLayout">
                <jsonFormatter class="ch.qos.logback.contrib.jackson.JacksonJsonFormatter"/>
                <appendLineSeparator>true</appendLineSeparator>
            </layout>
        </encoder>
    </appender>
----
The result is very close as well, but fields are named differently:
[source,json]
----
{
   "timestamp":"1511814391083",
   "level":"INFO",
   "thread":"Thread-1",
   "mdc":{"user":"gquintana"},
   "logger":"logodissey.DemoLogger",
   "message":"Hello world!",
   "context":"default"
}
----
Several Maven dependencies are required: `ch.qos.logback.contrib:logback-json-classic:0.1.5`, `ch.qos.logback.contrib:logback-jackson:0.1.5` and `com.fasterxml.jackson.core:jackson-databind`.

