= Ansible and rolling upgrades

:hp-tags: ansible, elasticsearch, kafka, cassandra
:hp-image: /images/logos/ansible.png
:source_dir: ../sources/2017-07-05-Ansible-and-rolling-upgrades
:image_dir: 2017-07-05-Ansible-and-rolling-upgrades
:published_at: 2017-01-15

Ansible is a nice tool to deploy distributed systems like Elasticsearch, Kafka, Cassandra and the like.
These distributed systems are built to avoid downtime and allow partial failures.
Upgrading these softwares, or updating their configuration, requires restarting each member of the cluster.

The aim of the this article is to describe an Ansible pattern I discovered
trying to improve deployment speed
and yet being able to guarantee availability during the upgrade process.
To improve deployment, you need to deploy all hosts in parallel.
To guarantee availability, you can't stop all host at the same time: you must deploy each host one after the other.

== The problem

Let's take the Elasticsearch example.

The tasks required to deploy Elasticsearch on each host of the cluster are gathered in an `elasticsearch` role.
Then this role is applied to all hosts belonging to the `elasticsearch` group in the Ansible playbook.

.playbook.yml
[source,yaml]
----
- hosts: elasticsearch
  any_errors_fatal: true
  roles:
    - role: elasticsearch
----

At some point, the `elasticsearch` role probably contains something to stop Elasticsearch in order to apply changes:

.roles/elasticsearch/tasks/main.yml
[source,yaml]
----
...
- name: Stop service
  become: true
  service:
    name: elasticsearch
    state: stopped
    enabled: true
...
----
If we do that, all Elasticsearch nodes will be stopped at the same time, making the whole cluster unavailable.
To fix this problem, the role can be ran in serial instead of parallel:

.deploy.yml
[source,yaml]
----
- hosts: es
  serial: 1 # Force serial execution
  roles:
    - role: elasticsearch
----
But deploying every host in serial takes a very long time and makes deployment endless.

== Refactoring the role

The main idea is to split the role into multiple steps, and do as much as possible in parallel:

1. *In Parallel*: Deploy system settings, software settings, software binaries.
  But don't stop anything and don't remove anything.
2. *In Serial*:
  * Stop the node
  * Install or upgrade the node as quickly as possible
  * Start the node
3. *In Parallel*: finish applying settings on running cluster and remove old files and binaries.

Then descibe each step in its own YAML file: `before.yml`, `stop_start.yml` and `after.yml`.
Finally, the role entry point `main.yml` will include the appropriate step using a variable named `elasticsearch_step`:

.roles/elasticsearch/tasks/main.yml
[source,yaml]
----
- name: "Running step {{ elasticsearch_step }}"
  include: "{{ elasticsearch_step }}.yml"
----

We can also create a fake step including all the steps (more on that later):

.roles/elasticsearch/tasks/all.yml
[source,yaml]
----
- include: "before.yml"
- include: "stop_start.yml"
- include: "after.yml"
----

== Refactoring the playbook

Now in the playbook we will call the role 3 times, each individual step being called independently.

.playbook.yml
[source,yaml]
----
# In parallel
- hosts: elasticsearch
  any_errors_fatal: true
  roles:
    - role: elasticsearch
      elasticsearch_step: "before"

# In serial
- hosts: elasticsearch
  any_errors_fatal: true
  serial: 1
  roles:
    - role: elasticsearch
      elasticsearch_step: "stop_start"

# In parallel
- hosts: elasticsearch
  any_errors_fatal: true
  roles:
    - role: elasticsearch
      elasticsearch_step: "after"
----

To install a brand new cluster, there is nothing to stop, and we don't fear anything.
In this particular case, this role can still be used, and the `stop_start` step can be called in parallel:

.playbook.yml
[source,yaml]
----
# New cluster
- hosts: elasticsearch
  any_errors_fatal: true
  roles:
    - role: elasticsearch
      elasticsearch_step: "before"
    - role: elasticsearch
      elasticsearch_step: "stop_start"
    - role: elasticsearch
      elasticsearch_step: "after"
----

Or even simpler, we can use `all` step:

.playbook.yml
[source,yaml]
----
# New cluster
- hosts: elasticsearch
  any_errors_fatal: true
  roles:
    - role: elasticsearch
      elasticsearch_step: "all"
----

== Hot configuration

Most of the time, I am running my Ansible playbook only to change settings that don't need nodes to be restarted.
The trick is here to detect in the `before` step whether nodes should be restarted or not:
Version upgrade? Configuration that can not be hot reloaded?
On lucky days, you can skip the expensive `stop_start` step and have a quick deploy.
