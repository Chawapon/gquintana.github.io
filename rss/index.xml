<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[JRald]]></title><description><![CDATA[Yet another blog about tech stuff]]></description><link>https://gquintana.github.io</link><generator>RSS for Node</generator><lastBuildDate>Sun, 16 Oct 2016 19:51:51 GMT</lastBuildDate><atom:link href="https://gquintana.github.io/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Kafka Java Client]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Apache Kafka is trendy software which mixes a message broker and an event log.
From the ground up, it&#8217;s a distributed solution designed for scalability and performance.
It was created by LinkedIn in 2011, it is now open-source and supported by the Confluent company.</p>
</div>
<div class="paragraph">
<p>For Java developers, until Kafka 0.8, there was only an intricate Scala API with Java bindings.
Since 0.9 there is a pure Java API which makes things simpler.
In this blog post we will discuss this API.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_creating_a_topic">Creating a Topic</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The <strong>topic</strong> is the place where messages are sent and where they are stored.
All messages send to Kafka are written to disk.</p>
</div>
<div class="paragraph">
<p>A topic is split into multiple <strong>partitions</strong>.
Each partition is usually placed on a different Kafka node.
Each partition can be <strong>replicated</strong> many times in order to tolerate node failures.
Among replicas, a leader is elected, it has the privilege of receiving messages first and sending messages to consumers.</p>
</div>
<div class="paragraph">
<p>A topic is automatically created when the first message arrives.
Alternatively, it may be manually created with the <code>kafka-topic.sh</code> command line tool:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>$ bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic newtopic --partitions 5 --replication-factor 2
Created topic "newtopic".
$ bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic newtopic
Topic:newtopic  PartitionCount:5        ReplicationFactor:2     Configs:
        Topic: newtopic Partition: 0    Leader: 2       Replicas: 2,3   Isr: 2,3
        Topic: newtopic Partition: 1    Leader: 3       Replicas: 3,1   Isr: 3,1
        Topic: newtopic Partition: 2    Leader: 1       Replicas: 1,2   Isr: 1,2
        Topic: newtopic Partition: 3    Leader: 2       Replicas: 2,1   Isr: 2,1
        Topic: newtopic Partition: 4    Leader: 3       Replicas: 3,2   Isr: 3,2</code></pre>
</div>
</div>
<div class="paragraph">
<p>In the above example, the topic has five partitions.
Partition 0 has two replicas: one on node 2, the other node 3.
The replica on node 2 is the leader.
You will notice the leader partitions are evenly distributed among this 3 node cluster.</p>
</div>
<div class="paragraph">
<p>Each partition is an independent log of events.
In this log, messages are <strong>sorted</strong> by their arrival order.
Each message is identified by its position in the log, this position is called <strong>offset</strong>.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://gquintana.github.io/images/2016-10-10-Kafka-Java-Client/kafka_topic.svg" alt="Topic" width="Partitions and Offsets">
</div>
</div>
<div class="paragraph">
<p>At the time of writing (Kafka 0.10.0), it is not possible to create or delete a Topic with the Kafka Client library.
By the way, this should change in the upcoming release (0.10.1).
Right now, you&#8217;ll have to stick with the forementioned command line tool, or use the Scala library which contains an <code>AdminUtils</code> class.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_message_record">Message / Record</h2>
<div class="sectionbody">
<div class="paragraph">
<p>A message is also called a <strong>record</strong> in the Kafka vocabulary.
It consists of:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>An optional <strong>key</strong>: it doesn&#8217;t guarantee message uniqueness, multiple messages may have the same key.</p>
</li>
<li>
<p>A <strong>value</strong>: the body, the main part of the message</p>
</li>
<li>
<p>A <strong>timestamp</strong> (since Kafka 0.10): when the message was created by the producer or recorded in the broker</p>
</li>
<li>
<p>An <strong>offset</strong>: a big number describing the position of the message in the log</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>When the key is provided, it is hashed and this hash is used to determine in which partition it should go.
Two messages with the same key will end in the same partition.
Messages having the same key can be merged together by an optional background process called compaction.</p>
</div>
<div class="paragraph">
<p>The key and the value can be of any type: <code>String</code>, <code>long</code>, <code>byte[]</code>&#8230;&#8203;
This is made possible by <strong>serializers</strong> and <strong>deserializers</strong> which are strategies to read and write anything from byte stream.
Some (de)serializers are provided for basic types,
and there are some third party (de)serializers to handle complex types.
For example, it&#8217;s possible to exchange plain objects written in <a href="https://github.com/confluentinc/schema-registry/tree/master/json-serializer">JSON</a> or Avro format.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_sending_messages">Sending messages</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Messages are sent to the Kafka broker using a <strong>producer</strong>.
The producer knows the distribution of topic partitions on nodes,
it will hash the record key and send the record directly to the appropriate partition/node.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://gquintana.github.io/images/2016-10-10-Kafka-Java-Client/kafka_producer.svg" alt="Producer">
</div>
</div>
<div class="paragraph">
<p>When no key is provided, the producer uses a random partition.
In short, the producer is able to load balance writes to all partitions, and associated nodes.</p>
</div>
<div class="paragraph">
<p>This producer is initialized and configured with some properties.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">Map&lt;String, Object&gt; producerConfig = new HashMap&lt;&gt;();
producerConfig.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "kafka1:9092,kafka2:9092"); <i class="conum" data-value="1"></i><b>(1)</b>
producerConfig.put(ProducerConfig.ACKS_CONFIG, "1"); <i class="conum" data-value="5"></i><b>(5)</b>
producerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class); <i class="conum" data-value="4"></i><b>(4)</b>
producerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
try (Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(producerConfig)) {
    ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;("the_topic", "the_key","The Message"); <i class="conum" data-value="2"></i><b>(2)</b>
    Future&lt;RecordMetadata&gt; futureMetadata = producer.send(record);<i class="conum" data-value="3"></i><b>(3)</b>
    RecordMetadata metadata = producer.get(); <i class="conum" data-value="5"></i><b>(5)</b>
}</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Connect to these Kafka connect nodes.
The first Kafka node to answer will give the full list of nodes.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Create the record/message</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Send the message.
By default, this operation is asynchronous and non blocking, it immediately returns a <code>Future</code></td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>The message and the key are written on the wire using the string serializer.</td>
</tr>
<tr>
<td><i class="conum" data-value="5"></i><b>5</b></td>
<td>Wait for message acknowledgement.
The <strong>Acks</strong> config indicates how many replicas should write the message to disk before returning an acknowledgement to the producer.</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>In a real world application, the producer should be instanciated only once and reused for the application lifespan.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_receiving_messages">Receiving messages</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Messages are received from the Kafka broker by a <strong>consumer</strong>.
A consumer is a process in charge for reading messages from a topic and dealing with them.
As an acknowledgement, the consumer writes the message offset back to the broker, it&#8217;s called <strong>offset commit</strong>.</p>
</div>
<div class="paragraph">
<p>A <strong>consumer group</strong> is a set of consumers distributed on multiple machines.
For a given topic and group, each partition gets read by a single consumer.
This prevents messages from being consumed twice in the consumer group.
On the contrary, a consumer can be in charge of several partitions.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://gquintana.github.io/images/2016-10-10-Kafka-Java-Client/kafka_consumer.svg" alt="Consumer and Consumer Group">
</div>
</div>
<div class="paragraph">
<p>The Kafka cluster tells each consumer which partition it should read from.
Each consumer takes care of its portion of topic.
As a result, consumers can work independently and in parallel,
and messages stored in a topic can be load balanced to consumers on many machines.
In case of consumer failure, Kafka reassigns the partitions to other consumers of the same group.</p>
</div>
<div class="paragraph">
<p>Like the producer, the consumer is initialized and configured with some properties.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">Map&lt;String, Object&gt; consumerConfig = new HashMap&lt;&gt;();
consumerConfig.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "kafka1:9092,kafka2:9092"); <i class="conum" data-value="1"></i><b>(1)</b>
consumerConfig.put(ConsumerConfig.GROUP_ID_CONFIG, "test_group");
consumerConfig.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
consumerConfig.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
try (Consumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(consumerConfig)) {
    consumer.subscribe(Arrays.asList("the_topic")); <i class="conum" data-value="2"></i><b>(2)</b>
    ConsumerRecords&lt;String, String&gt; records = consumer.poll(1000L); <i class="conum" data-value="3"></i><b>(3)</b>
    for (ConsumerRecord&lt;String, String&gt; record : records) {
        LOGGER.info("Found message {} {}", record.key(), record.value());
    }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Connect to these Kafka connect nodes.
Like the producer, it doesn&#8217;t have to be the whole Kafka cluster.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Register this application (consumer group) as a consumer for this list of topics.
In return, Kafka will assign some partitions to this consumer.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Try to pull messages from the broker.</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>Pulled messages are automatically acknowledged.</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>In the above example, connecting to the broker, subscribing to one or more topics,
and being assigned partitions takes time and is usually done once during application start-up.
On the contrary, the <code>poll</code> method should be run in loop.
It returns a batch of records whose size is controlled by the <code>max.poll.records</code> and <code>max.partition.fetch.bytes</code> settings.</p>
</div>
<div class="paragraph">
<p>Unlike the producer, the consumer is not thread-safe.
In order to consume records in parallel, each thread should have it&#8217;s own consumer.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_acknowledging_received_messages">Acknowledging received messages</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The message acknowledgement is called <strong>offset commit</strong>,
because Kafka keeps track of the offset of the last consumed message for each topic + partition + consumer group.</p>
</div>
<div class="paragraph">
<p>In the previous example, the offsets were automatically and periodically committed to the broker.
This auto commit is configurable through properties:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">consumerConfig.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true);
consumerConfig.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, 1000L);
try (Consumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(consumerConfig)) {
    consumer.subscribe(Arrays.asList("the_topic"));
    ConsumerRecords&lt;String, String&gt; records = consumer.poll(1000L);
    for (ConsumerRecord&lt;String, String&gt; record : records) {
        LOGGER.info("Found message {} {}", record.key(), record.value());
    }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>This offset commit can also be manual in order to ensure messages are acknowledged once they have been processed.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">consumerConfig.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);
try (Consumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(consumerConfig)) {
    consumer.subscribe(Arrays.asList("the_topic"));
    ConsumerRecords&lt;String, String&gt; records = consumer.poll(1000L);
    for (ConsumerRecord&lt;String, String&gt; record : records) {
        LOGGER.info("Found message {} {}", record.key(), record.value());
    }
    consumer.commitSync();
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>This offset can even be moved forward (to skip records) and backward (to replay records):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">    consumer.seekToBeginning(consumer.assignment());</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_using_a_framework">Using a framework</h2>
<div class="sectionbody">
<div class="paragraph">
<p>You may have noticed that the consumer API is a pull API.
In a real application you&#8217;ll have to create a consuming loop in separate thread,
and build a push API.</p>
</div>
<div class="paragraph">
<p>The <a href="http://docs.spring.io/spring-kafka/docs/current/reference/html/">Spring Kafka</a> does all the heavy lifting for you
and smoothly integrates Kafka with Spring and Spring Integration:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The <code>KafkaTemplate</code> can send messages</p>
</li>
<li>
<p>The <code>KafkaListener</code> can receive message in a push manner</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This library makes Kafka usage very similar to ActiveMQ or RabbitMQ.</p>
</div>
</div>
</div>]]></description><link>https://gquintana.github.io/2016/10/10/Kafka-Java-Client.html</link><guid isPermaLink="true">https://gquintana.github.io/2016/10/10/Kafka-Java-Client.html</guid><category><![CDATA[kafka]]></category><category><![CDATA[ java]]></category><dc:creator><![CDATA[Gerald Quintana]]></dc:creator><pubDate>Mon, 10 Oct 2016 00:00:00 GMT</pubDate></item><item><title><![CDATA[Testing Logstash configuration]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>You wrote a piece of Logstash configuration which can parse some logs.
You tested several corner cases to ensure the output in Elasticsearch was alright.
How do you protect this clever configuration file against regressions?</p>
</div>
<div class="paragraph">
<p>Unit testing to the rescue of course!</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_simple_example">Simple example</h2>
<div class="sectionbody">
<div class="paragraph">
<p>For the sake of simplicity, we will take an obvious example: access logs.
The input looks like</p>
</div>
<div class="listingblock">
<div class="content">
<pre>172.17.0.1 - - [05/Sep/2016:20:06:17 +0000] "GET /images/logos/hubpress.png HTTP/1.1" 200 5432 "http://localhost/" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/51.0.2704.79 Chrome/51.0.2704.79 Safari/537.36" "-"</pre>
</div>
</div>
<div class="paragraph">
<p>The output, once in Elasticsearch, should look like</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">{ "@version":"1",
  "@timestamp":"2016-09-05T20:06:17.000Z",
  "type":"nginx",
  "host":"nginx-server", "path":"/var/log/nginx/access.log",
  "clientip":"172.17.0.1", "ident":"-", "auth":"-",
  "verb":"GET","request":"/images/logos/hubpress.png","httpversion":"1.1",
  "response":200, "bytes":5432, "referrer":"\"http://localhost/\"",
  "agent": "\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/51.0.2704.79 Chrome/51.0.2704.79 Safari/537.36\""
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The configuration could look like</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-ruby" data-lang="ruby">input {
    file {
        path =&gt; "/var/log/nginx/access*.log"
        type =&gt; "nginx"
    }
}
filter {
    if [type] == "nginx" {
        grok {
            match =&gt; [ "message" , "%{COMBINEDAPACHELOG}"]
        }
        date {
            match =&gt; [ "timestamp" , "dd/MMM/YYYY:HH:mm:ss Z" ]
        }
        mutate {
            convert =&gt; ["response", "integer"]
            convert =&gt; ["bytes", "integer"]
        }
    }
}
output {
    elasticsearch {
      hosts =&gt; [ "es-server"]
      index =&gt; "logstash-%{+YYYY.MM.dd}"
      document_type =&gt; "%{type}"
    }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_split_the_file">Split the file</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In the above config file, the interesting part, the one containing logic is the filter part.
In order to test it, the first thing to do is split this big file into small pieces:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>01_logstash_input_nginx.conf</code> contains the nginx file input</p>
</li>
<li>
<p><code>02_logstash_filter_nginx.conf</code> contains the nginx filter section</p>
</li>
<li>
<p><code>03_logstash_output.conf</code> contains the elasticsearch output</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In production, you can load multiple config files as if they were a single one:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>logstash agent -f /etc/logstash.d/*.conf"</pre>
</div>
</div>
<div class="paragraph">
<p>At test time, by picking a single configuration file <code>02_logstash_filter_nginx.conf</code>, the Nginx log parsing can be tested in isolation.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_write_the_unit_test">Write the unit test</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Now let&#8217;s test the <code>02_logstash_filter_nginx.conf</code> file alone and write a simple Ruby test case.
As you may know, Logstash is written in JRuby.</p>
</div>
<div class="listingblock">
<div class="title">02_logstash_filter_nginx_spec.rb</div>
<div class="content">
<pre class="highlight"><code class="language-ruby" data-lang="ruby"># encoding: utf-8
require "logstash/devutils/rspec/spec_helper"

# Load the configuration file
@@configuration = String.new
@@configuration &lt;&lt; File.read("conf/02_logstash_nginx_filter.conf")

describe "Nginx filter" do

  config(@@configuration) <i class="conum" data-value="1"></i><b>(1)</b>

  # Inject input event/message into the pipeline
  message = "172.17.0.1 - - [05/Sep/2016:20:06:17 +0000] \"GET /images/logos/hubpress.png HTTP/1.1\" 200 5432 \"http://localhost/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/51.0.2704.79 Chrome/51.0.2704.79 Safari/537.36\" \"-\""
  sample("message" =&gt; message, "type" =&gt; "nginx") do <i class="conum" data-value="2"></i><b>(2)</b>
    # Check the ouput event/message properties
    insist { subject.get("type") } == "nginx" <i class="conum" data-value="3"></i><b>(3)</b>
    insist { subject.get("@timestamp").to_iso8601 } == "2016-09-05T20:06:17.000Z"
    insist { subject.get("verb") } == "GET"
    insist { subject.get("request") } == "/images/logos/hubpress.png"
    insist { subject.get("response") } == 200
    insist { subject.get("bytes") } == 5432
    reject { subject.get("tags").include?("_grokparsefailure") }
    reject { subject.get("tags").include?("_dateparsefailure") }
  end
end</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Load configuration file</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Inject input event/message into the pipeline</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Check the ouput event/message properties</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>This test uses the JRuby testing framework called RSpec (<code>describe</code> method).
The <code>config</code> and <code>sample</code> functions are located in the <a href="https://github.com/elastic/logstash-devutils">Logstash DevUtils</a> library.
The <code>insist</code> and <code>reject</code> functions are part of the <a href="https://github.com/jordansissel/ruby-insist">Ruby Insist</a> assertion library.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_run_the_unit_tests">Run the unit tests</h2>
<div class="sectionbody">
<div class="paragraph">
<p>First we will need to download and install additional development libraries like those mentioned above.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ logstash-2.4.0/bin/logstash-plugin install --development
Installing logstash-devutils, logstash-input-generator, logstash-codec-json, logstash-output-null, logstash-filter-mutate, flores, rspec, stud, pry, rspec-wait, childprocess, ftw, logstash-output-elasticsearch, rspec-sequencing, gmetric, gelf, timecop, jdbc-derby, docker-api, logstash-codec-plain, simplecov, coveralls, longshoreman, rumbster, logstash-filter-kv, logstash-filter-ruby, sinatra, webrick, poseidon, logstash-output-lumberjack, webmock, logstash-codec-line, logstash-filter-grok
Installation successful</pre>
</div>
</div>
<div class="paragraph">
<p>Now we can run the test, Logstash comes with a <code>rspec</code> command to run these spec files.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ logstash-2.4.0/bin/rspec 02_logstash_nginx_filter_spec.rb
Using Accessor#strict_set for specs
Run options: exclude {:redis=&gt;true, :socket=&gt;true, :performance=&gt;true, :couchdb=&gt;true, :elasticsearch=&gt;true, :elasticsearch_secure=&gt;true, :export_cypher=&gt;true, :integration=&gt;true, :windows=&gt;true}
.

Finished in 0.115 seconds (files took 0.784 seconds to load)
1 example, 0 failures

Randomized with seed 4384</pre>
</div>
</div>
<div class="paragraph">
<p>The <code>rspec</code> command can also run multiple tests at once.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ logstash-2.4.0/bin/rspec spec -P '**/*_spec.rb'</pre>
</div>
</div>
<div class="paragraph">
<p>To prevent test dependencies, they are randomly ordered: This called randomized testing.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_give_me_the_code">Give me the code!</h2>
<div class="sectionbody">
<div class="paragraph">
<p>All the code shown in this article is available in <a href="https://github.com/gquintana/gquintana.github.io/tree/master/sources/2016-09-07-Testing-Logstash-configuration">Github</a>.</p>
</div>
</div>
</div>]]></description><link>https://gquintana.github.io/2016/09/07/Testing-Logstash-configuration.html</link><guid isPermaLink="true">https://gquintana.github.io/2016/09/07/Testing-Logstash-configuration.html</guid><category><![CDATA[logstash]]></category><dc:creator><![CDATA[Gerald Quintana]]></dc:creator><pubDate>Wed, 07 Sep 2016 00:00:00 GMT</pubDate></item><item><title><![CDATA[Securing remote JMX]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Like it or not, JMX is one the main tools for JVM monitoring.
If you are using Tomcat, Kafka or Cassandra you&#8217;ll have to setup JMX tools to monitor them.</p>
</div>
<div class="paragraph">
<p>But JMX has several drawbacks:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>It&#8217;s based on the obsolete RMI protocol</p>
</li>
<li>
<p>It can trigger harmful functions like garbage collection</p>
</li>
<li>
<p>It can be used for nasty exploits like <a href="https://issues.apache.org/jira/browse/COLLECTIONS-580">invoking arbitrary code</a></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>So securing JMX is not an option.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_going_remote">Going remote</h2>
<div class="sectionbody">
<div class="paragraph">
<p>By default, JMX is only locally accessible and secure: It can be accessed through Unix sockets.
This means you need to have access to the machine and run JMX tools with the same user as your application.
It&#8217;s usually enough for development but not for production.</p>
</div>
<div class="paragraph">
<p>To enable remote JMX, the documentations tells you turn on some JVM flags:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>java -Dcom.sun.management.jmxremote=true \
     -Dcom.sun.management.jmxremote.port=31419 \
     -Dcom.sun.management.jmxremote.ssl=false \
     -Dcom.sun.management.jmxremote.authenticate=false \
     Tomcat</pre>
</div>
</div>
<div class="paragraph">
<p>The JVM will starting listening on <strong>0.0.0.0:31419</strong> for JMX requests.
Anybody will be able to plug any JMX tool (JConsole, JVisualVM, Mission Control&#8230;&#8203;) from a remote machine.</p>
</div>
<div class="paragraph">
<p>If you have a firewall (IPTable or the like) to protect your server, and opened the 31419 TCP port,
or connecting through a tunnel, the JMX tools may fail to connect to the server.
At first, the JMX client connects to port <strong>31419</strong>, but gets redirected to a randomly chosen port (RMI WTF!).
To prevent this behaviour and force the JVM to use a single port:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>java -Dcom.sun.management.jmxremote=true \
     -Dcom.sun.management.jmxremote.port=31419 \
     -Dcom.sun.management.jmxremote.rmi.port=31419 \
     -Dcom.sun.management.jmxremote.ssl=false \
     -Dcom.sun.management.jmxremote.authenticate=false \
     Tomcat</pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_file_based_authentication">File based authentication</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This configuration is nice for debugging but not secure at all.
Let&#8217;s add authentication on this JMX connection.</p>
</div>
<div class="paragraph">
<p>First create a <strong>password</strong> file, similar to <strong>/etc/password</strong>, containing login/password pairs:</p>
</div>
<div class="listingblock">
<div class="title">jmxremote.password</div>
<div class="content">
<pre class="highlight"><code>admin  adminpassword
user   userpassword</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then create an <strong>access</strong> file, similar to <strong>/etc/groups</strong>, containing login/group pairs:</p>
</div>
<div class="listingblock">
<div class="title">jmxremote.access</div>
<div class="content">
<pre class="highlight"><code>admin readwrite <i class="conum" data-value="1"></i><b>(1)</b>
user  readonly <i class="conum" data-value="2"></i><b>(2)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>admin has read/write access</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>user  has read-only access</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>These two files should have limited access rights.</p>
</div>
<div class="paragraph">
<p>Finally, tell the JVM to use these files to authenticate users:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>java -Dcom.sun.management.jmxremote=true \
     -Dcom.sun.management.jmxremote.port=31419 \
     -Dcom.sun.management.jmxremote.rmi.port=31419 \
     -Dcom.sun.management.jmxremote.ssl=false \
     -Dcom.sun.management.jmxremote.password.file=/path/to/jmxremote.password \
     -Dcom.sun.management.jmxremote.access.file=/path/to/jmxremote.access \
     Tomcat</pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_using_ssl">Using SSL</h2>
<div class="sectionbody">
<div class="paragraph">
<p>With this configuration, JVM tools have to provide a login and password to access to MBeans.
But, the password is sent over the wire without encryption.
Let&#8217;s connect to JMX though SSL.</p>
</div>
<div class="paragraph">
<p>You may already known that, in the Java land,
private keys and trusted certificates are stored in wallets known as <strong>keystores</strong>,
and using the <strong>JKS</strong> file format.
A tool named <strong>keytool</strong> provided with the JDK is used to import/export keys and certs in the keystore.</p>
</div>
<div class="paragraph">
<p>Once the <strong>keystore</strong> (containing private key matching the server name along with certificate chain)
and the <strong>truststore</strong> (containing trusted certificates) are built,
just reference them:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>java -Dcom.sun.management.jmxremote=true \
     -Dcom.sun.management.jmxremote.port=31419 \
     -Dcom.sun.management.jmxremote.rmi.port=31419 \
     -Dcom.sun.management.jmxremote.password.file=/path/to/jmxremote.password \
     -Dcom.sun.management.jmxremote.access.file=/path/to/jmxremote.access \
     -Dcom.sun.management.jmxremote.registry.ssl=true \
     -Djavax.net.ssl.keyStore=/path/to/keystore.jks \
     -Djavax.net.ssl.keyStorePassword=keystore_password \
     -Djavax.net.ssl.trustStore=/path/to/truststore.jks \
     -Djavax.net.ssl.trustStorePassword=truststore_password \
     Tomcat</pre>
</div>
</div>
<div class="paragraph">
<p>To connect, the JMX tools using SSL, you&#8217;ll have to provide the trusted certificates.
For instance, to start the <strong>JConsole</strong>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>jconsole \
     -J-Djavax.net.ssl.trustStore=/path/to/truststore.jks \
     -J-Djavax.net.ssl.trustStorePassword=truststore_password</pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_jmx_configuration_file">JMX configuration file</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Again, this configuration has problem:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Having passwords on the command line is not a good option,
because it&#8217;s easy to use <code>ps</code> command to grab them.</p>
</li>
<li>
<p>Having some many options on the command like is not elegant</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Let&#8217;s place all these options in a dedicated file:</p>
</div>
<div class="listingblock">
<div class="title">jmxremote.properties</div>
<div class="content">
<pre>com.sun.management.jmxremote=true
com.sun.management.jmxremote.port=31419
com.sun.management.jmxremote.rmi.port=31419
com.sun.management.jmxremote.password.file=/path/to/jmxremote.password
com.sun.management.jmxremote.access.file=/path/to/jmxremote.access
com.sun.management.jmxremote.registry.ssl=true
com.sun.management.jmxremote.ssl.config.file=/path/to/jmxremote.properties <i class="conum" data-value="1"></i><b>(1)</b>

javax.net.ssl.keyStore=/path/to/keystore.jks
javax.net.ssl.keyStorePassword=keystore_password
javax.net.ssl.trustStore=/path/to/truststore.jks
javax.net.ssl.trustStorePassword=truststore_password</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Path to file containing <code>javax.net.ssl.*</code> properties</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>And now, the command line sums up to</p>
</div>
<div class="listingblock">
<div class="content">
<pre>java -Dcom.sun.management.config.file=/path/to/jmxremote.properties \
     Tomcat</pre>
</div>
</div>
<div class="paragraph">
<p>Such a JMX configuration file already exists in your JRE, it&#8217;s named <code>JRE/lib/management/management.properties</code>.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_some_pointers">Some pointers</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p><a href="https://docs.oracle.com/javase/8/docs/technotes/guides/management/">Java 8 JMX</a></p>
</li>
<li>
<p><a href="https://www.jtips.info/index.php?title=JMX/Remote">JTips</a> in French</p>
</li>
</ul>
</div>
</div>
</div>]]></description><link>https://gquintana.github.io/2016/09/01/Securing-remote-JMX.html</link><guid isPermaLink="true">https://gquintana.github.io/2016/09/01/Securing-remote-JMX.html</guid><category><![CDATA[java]]></category><dc:creator><![CDATA[Gerald Quintana]]></dc:creator><pubDate>Thu, 01 Sep 2016 00:00:00 GMT</pubDate></item><item><title><![CDATA[Elasticsearch and YAML]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Most examples in Elasticsearch documentation are using JSON to represent documents, requests and responses.
Writing JSON is not that hard but you sometimes become Raiders of the Lost Curly Brace.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://upload.wikimedia.org/wikipedia/en/4/4c/Raiders_of_the_Lost_Ark.jpg" alt="Raiders of the Lost Ark">
</div>
</div>
<div class="paragraph">
<p>Do you know that you can replace JSON by YAML?</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_in_responses">In responses</h2>
<div class="sectionbody">
<div class="paragraph">
<p>You can add <code>format=yaml</code> in the query params:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>curl -XGET http://localhost:9200/logstash-*/_search?format=yaml

---
took: 13
timed_out: false
_shards:
  total: 15
  successful: 15
  failed: 0
hits:
  total: 2382
  max_score: 1.0
  hits:
  - _index: "logstash-2016.06.22"
    _type: "test"
    _id: "AVV36B3vd6qInGJw3x0r"
    _score: 1.0
    _source:
      '@timestamp': "2016-06-22T11:39:07.431Z"
  - _index: "logstash-2016.06.22"
    _type: "test"
    _id: "AVV36Cfrd6qInGJw3x0u"
    _score: 1.0
    _source:
      '@timestamp': "2016-06-22T11:38:42.000Z"</pre>
</div>
</div>
<div class="paragraph">
<p>You can also use the <code>Content-Type: application/yaml</code> HTTP header:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>curl -XGET -H "Content-Type: application/yaml" http://localhost:9200/_cluster/health

---
cluster_name: "elasticsearch"
status: "yellow"
timed_out: false
number_of_nodes: 1
number_of_data_nodes: 1
...</pre>
</div>
</div>
<div class="paragraph">
<p>While using YAML format for responses you don&#8217;t have to use the <code>pretty=true</code> query param,
as YAML is naturally indented and human readable.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_in_requests">In requests</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Just start your request body with <code>---</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>curl -XPOST http://localhost:9200/_search -d '---
query:
  match:
    message: "Elasticsearch"
'</pre>
</div>
</div>
<div class="paragraph">
<p>This gets pretty handy when dealing with mappings and settings:</p>
</div>
<div class="listingblock">
<div class="title">movie.mapping.yaml</div>
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">---
properties:
  title:
    type: string
  tags:
    type: string
    index: not_analyzed
  year:
    type: date
    format: year</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>curl -XPUT http://localhost:9200/movies/_mappings/movie --data-binary @movie.mapping.yaml</pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_limits">Limits</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Some requests like <code>_bulk</code> request may not accept YAML.</p>
</div>
<div class="paragraph">
<p>Sense (Kibana Console) and other Elasticsearch UIs do not handle YAML properly.</p>
</div>
</div>
</div>]]></description><link>https://gquintana.github.io/2016/08/25/Elasticsearch-and-YAML.html</link><guid isPermaLink="true">https://gquintana.github.io/2016/08/25/Elasticsearch-and-YAML.html</guid><category><![CDATA[elasticsearch]]></category><dc:creator><![CDATA[Gerald Quintana]]></dc:creator><pubDate>Thu, 25 Aug 2016 00:00:00 GMT</pubDate></item><item><title><![CDATA[First post]]></title><description><![CDATA[<div class="paragraph">
<p>This is my first post on this blog.</p>
</div>
<div class="paragraph">
<p>I am trying out <a href="http://hubpress.io">HubPress.io</a> and I&#8217;m just amazed:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>5 minutes to setup un blog site on GitHub.</p>
</li>
<li>
<p>A neat backoffice to administrate it</p>
</li>
<li>
<p>Posts written in AsciiDoc with live preview.</p>
</li>
<li>
<p>Built with cool tech: React, Redux,</p>
</li>
<li>
<p>Mostly built in France: cock-a-doodle-doo</p>
</li>
</ul>
</div>]]></description><link>https://gquintana.github.io/2016/08/23/First-post.html</link><guid isPermaLink="true">https://gquintana.github.io/2016/08/23/First-post.html</guid><category><![CDATA[mood]]></category><dc:creator><![CDATA[Gerald Quintana]]></dc:creator><pubDate>Tue, 23 Aug 2016 00:00:00 GMT</pubDate></item></channel></rss>